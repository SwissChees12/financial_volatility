{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "round-arthritis",
   "metadata": {},
   "source": [
    "This notebook is intended to extract the csv files and split them into individual files for each day. All these files will be stored in data/data_per_day folder and each file name will be of the form ``SPY_20200203.csv``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "appointed-detail",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required libraries\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import gzip\n",
    "import shutil\n",
    "from timeit import default_timer as timer\n",
    "# Paths\n",
    "sys.path.append(os.path.join(Path(os.getcwd()).parent))  \n",
    "data_path = os.path.join(os.path.join(Path(os.getcwd()).parent), 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sufficient-fitting",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/patricklucescu/Dropbox/Projects/financial_volatility/financial_volatility/data/ezu_may2019.gz',\n",
       " '/home/patricklucescu/Dropbox/Projects/financial_volatility/financial_volatility/data/iwm_may2019.gz',\n",
       " '/home/patricklucescu/Dropbox/Projects/financial_volatility/financial_volatility/data/eem_may2019.gz',\n",
       " '/home/patricklucescu/Dropbox/Projects/financial_volatility/financial_volatility/data/vea_may2019.gz',\n",
       " '/home/patricklucescu/Dropbox/Projects/financial_volatility/financial_volatility/data/spy_may2019.gz']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get data location\n",
    "zip_files = list()\n",
    "zip_files = [f for f in os.listdir(data_path) if os.path.isfile(os.path.join(data_path, f))]\n",
    "zip_files = [file for file in zip_files if '.gz' in file]\n",
    "zip_files = [os.path.join(data_path, x) for x in zip_files]\n",
    "zip_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "weird-pennsylvania",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_file = zip_files[4]\n",
    "with gzip.open(zip_file, 'rb') as f_in:\n",
    "        with open(os.path.join(data_path, 'test.csv'), 'wb') as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "# get the csv file now\n",
    "csv_file = os.path.join(data_path, 'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "tropical-summer",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 20190501 in 26.97730266200233 seconds\n",
      "Finished 20190502 in 44.063456748001045 seconds\n",
      "Finished 20190503 in 23.704169062999426 seconds\n",
      "Finished 20190506 in 34.76921290500104 seconds\n",
      "Finished 20190507 in 55.29991199000142 seconds\n",
      "Finished 20190508 in 30.11882609500026 seconds\n",
      "Finished 20190509 in 47.95164199999999 seconds\n",
      "Finished 20190510 in 44.879307227001846 seconds\n",
      "Finished 20190513 in 59.501354406002065 seconds\n",
      "Finished 20190514 in 30.685433705999458 seconds\n",
      "Finished 20190515 in 29.253609355000663 seconds\n",
      "Finished 20190516 in 26.471355304998724 seconds\n",
      "Finished 20190517 in 43.02775007799937 seconds\n",
      "Finished 20190520 in 35.85772383699805 seconds\n",
      "Finished 20190521 in 27.664010668999254 seconds\n",
      "Finished 20190522 in 25.47703297100088 seconds\n",
      "Finished 20190523 in 47.60961755600147 seconds\n",
      "Finished 20190524 in 26.81840456899954 seconds\n",
      "Finished 20190528 in 28.921080643001915 seconds\n",
      "Finished 20190529 in 37.74888269099756 seconds\n",
      "Finished 20190530 in 28.333298380999622 seconds\n",
      "Finished 20190531 in 32.509257001998776 seconds\n",
      "CPU times: user 11min 53s, sys: 1min 14s, total: 13min 7s\n",
      "Wall time: 13min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_parser = pd.read_csv(csv_file, chunksize=5*10**5)\n",
    "start=timer()\n",
    "for df in data_parser:\n",
    "    #print(1)\n",
    "    dates = np.sort(df.DATE.unique())\n",
    "    if 'current_date' not in locals():\n",
    "        current_date = dates[0]\n",
    "    if 'ticker' not in locals():\n",
    "        ticker = df.SYM_ROOT.values[0]\n",
    "    \n",
    "    if len(dates)==1 and current_date==dates[0]:  # only one date which is the current one\n",
    "        if 'df_date' not in locals():\n",
    "            df_date = df \n",
    "        else:\n",
    "            df_date = pd.concat([df_date, df])\n",
    "    elif len(dates)==1 and current_date!=dates[0]:  # only one date but it is not the current date\n",
    "        df_date.DATE = df_date.DATE.astype(str).apply(lambda x: x[:4]+'-'+x[4:6]+'-'+x[6:])\n",
    "        df_date['DT'] = df_date.DATE + ' ' +df_date.TIME_M\n",
    "        df_date.drop(columns=['DATE', 'TIME_M'], inplace=True)\n",
    "        df_date.to_csv(os.path.join(data_path, 'data_per_day',f'{ticker}', f'{ticker}_{current_date}.csv'), index=False)\n",
    "        end=timer()\n",
    "        print(f'Finished {current_date} in {end-start} seconds')\n",
    "        start=timer()\n",
    "        df_date = df\n",
    "        current_date = dates[0]\n",
    "    elif len(dates)==2:\n",
    "        df_date = pd.concat([df_date, df[df.DATE == current_date]])\n",
    "        df_date.DATE = df_date.DATE.astype(str).apply(lambda x: x[:4]+'-'+x[4:6]+'-'+x[6:])\n",
    "        df_date['DT'] = df_date.DATE + ' ' +df_date.TIME_M\n",
    "        df_date.drop(columns=['DATE', 'TIME_M'], inplace=True)\n",
    "        df_date.to_csv(os.path.join(data_path, 'data_per_day',f'{ticker}', f'{ticker}_{current_date}.csv'), index=False)\n",
    "        end=timer()\n",
    "        print(f'Finished {current_date} in {end-start} seconds')\n",
    "        start=timer()\n",
    "        current_date = dates[1]\n",
    "        df_date = df[df.DATE == current_date]\n",
    "    else:\n",
    "        print('More than two dates are present. Chuncksize is too big!')\n",
    "        break\n",
    "df_date.DATE = df_date.DATE.astype(str).apply(lambda x: x[:4]+'-'+x[4:6]+'-'+x[6:])\n",
    "df_date['DT'] = df_date.DATE + ' ' +df_date.TIME_M\n",
    "df_date.drop(columns=['DATE', 'TIME_M'], inplace=True)\n",
    "df_date.to_csv(os.path.join(data_path, 'data_per_day',f'{ticker}', f'{ticker}_{current_date}.csv'), index=False) # as the last chuck is processed but never saved\n",
    "end=timer()\n",
    "print(f'Finished {current_date} in {end-start} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "literary-sector",
   "metadata": {},
   "source": [
    "Several cases:\n",
    "    - only one date which is the current one\n",
    "        - either create or append to df_date\n",
    "    - only one date but it is not the current date\n",
    "        - save the df_date to csv and redifine df_date = df\n",
    "    - two dates:\n",
    "        - truncate, append & save & redifine df_date =  df and the current date\n",
    "    - more than two dates:\n",
    "        - chunck is too big \n",
    "        \n",
    "   Only issue remaining is the last chuck! which you do not save"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

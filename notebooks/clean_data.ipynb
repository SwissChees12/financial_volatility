{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "assumed-strategy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required libraries\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import zipfile \n",
    "from timeit import default_timer as timer\n",
    "import sqlalchemy as db\n",
    "# Paths\n",
    "sys.path.append(os.path.join(Path(os.getcwd()).parent))  \n",
    "data_path = os.path.join(os.path.join(Path(os.getcwd()).parent), 'data')\n",
    "data_per_day_path = os.path.join(os.path.join(Path(os.getcwd()).parent), 'data','data_per_day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "requested-chaos",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sqllite database\n",
    "# create database\n",
    "import sqlite3\n",
    "con = sqlite3.connect(os.path.join(data_path, 'database.db'))\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "alike-fight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create connection to sqlite database\n",
    "db_path = os.path.join(data_path, 'database.db')\n",
    "db_engine = db.create_engine('sqlite:///' + db_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "alive-piano",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/patricklucescu/Dropbox/Projects/financial_volatility/financial_volatility/data/data_per_day/SPY',\n",
       " '/home/patricklucescu/Dropbox/Projects/financial_volatility/financial_volatility/data/data_per_day/IWM',\n",
       " '/home/patricklucescu/Dropbox/Projects/financial_volatility/financial_volatility/data/data_per_day/EEM',\n",
       " '/home/patricklucescu/Dropbox/Projects/financial_volatility/financial_volatility/data/data_per_day/EZU',\n",
       " '/home/patricklucescu/Dropbox/Projects/financial_volatility/financial_volatility/data/data_per_day/VEA']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the data folders file now\n",
    "data_folders = [f for f in os.listdir(data_per_day_path) if not os.path.isfile(os.path.join(data_per_day_path, f))]\n",
    "data_folders = [file for file in data_folders if '.' not in file]\n",
    "data_folders = [os.path.join(data_per_day_path, x) for x in data_folders]\n",
    "data_folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "illegal-cotton",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['/home/patricklucescu/Dropbox/Projects/financial_volatility/financial_volatility/data/data_per_day/VEA/VEA_20190501.csv',\n",
       "       '/home/patricklucescu/Dropbox/Projects/financial_volatility/financial_volatility/data/data_per_day/VEA/VEA_20190502.csv',\n",
       "       '/home/patricklucescu/Dropbox/Projects/financial_volatility/financial_volatility/data/data_per_day/VEA/VEA_20190503.csv',\n",
       "       '/home/patricklucescu/Dropbox/Projects/financial_volatility/financial_volatility/data/data_per_day/VEA/VEA_20190506.csv',\n",
       "       '/home/patricklucescu/Dropbox/Projects/financial_volatility/financial_volatility/data/data_per_day/VEA/VEA_20190507.csv',\n",
       "       '/home/patricklucescu/Dropbox/Projects/financial_volatility/financial_volatility/data/data_per_day/VEA/VEA_20190508.csv',\n",
       "       '/home/patricklucescu/Dropbox/Projects/financial_volatility/financial_volatility/data/data_per_day/VEA/VEA_20190509.csv',\n",
       "       '/home/patricklucescu/Dropbox/Projects/financial_volatility/financial_volatility/data/data_per_day/VEA/VEA_20190510.csv',\n",
       "       '/home/patricklucescu/Dropbox/Projects/financial_volatility/financial_volatility/data/data_per_day/VEA/VEA_20190513.csv',\n",
       "       '/home/patricklucescu/Dropbox/Projects/financial_volatility/financial_volatility/data/data_per_day/VEA/VEA_20190514.csv',\n",
       "       '/home/patricklucescu/Dropbox/Projects/financial_volatility/financial_volatility/data/data_per_day/VEA/VEA_20190515.csv',\n",
       "       '/home/patricklucescu/Dropbox/Projects/financial_volatility/financial_volatility/data/data_per_day/VEA/VEA_20190516.csv',\n",
       "       '/home/patricklucescu/Dropbox/Projects/financial_volatility/financial_volatility/data/data_per_day/VEA/VEA_20190517.csv',\n",
       "       '/home/patricklucescu/Dropbox/Projects/financial_volatility/financial_volatility/data/data_per_day/VEA/VEA_20190520.csv',\n",
       "       '/home/patricklucescu/Dropbox/Projects/financial_volatility/financial_volatility/data/data_per_day/VEA/VEA_20190521.csv',\n",
       "       '/home/patricklucescu/Dropbox/Projects/financial_volatility/financial_volatility/data/data_per_day/VEA/VEA_20190522.csv',\n",
       "       '/home/patricklucescu/Dropbox/Projects/financial_volatility/financial_volatility/data/data_per_day/VEA/VEA_20190523.csv',\n",
       "       '/home/patricklucescu/Dropbox/Projects/financial_volatility/financial_volatility/data/data_per_day/VEA/VEA_20190524.csv',\n",
       "       '/home/patricklucescu/Dropbox/Projects/financial_volatility/financial_volatility/data/data_per_day/VEA/VEA_20190528.csv',\n",
       "       '/home/patricklucescu/Dropbox/Projects/financial_volatility/financial_volatility/data/data_per_day/VEA/VEA_20190529.csv',\n",
       "       '/home/patricklucescu/Dropbox/Projects/financial_volatility/financial_volatility/data/data_per_day/VEA/VEA_20190530.csv',\n",
       "       '/home/patricklucescu/Dropbox/Projects/financial_volatility/financial_volatility/data/data_per_day/VEA/VEA_20190531.csv'],\n",
       "      dtype='<U118')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the csv file now\n",
    "data_folder = data_folders[4]\n",
    "table_name = data_folder[-3:]\n",
    "csv_files = [f for f in os.listdir(data_folder) if os.path.isfile(os.path.join(data_folder, f))]\n",
    "csv_files = [file for file in csv_files if '.csv' in file]\n",
    "csv_files = np.sort([os.path.join(data_folder, x) for x in csv_files])\n",
    "csv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "objective-custom",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'VEA'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_folder[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "prerequisite-frequency",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start with day-----2019-05-01\n",
      "   Finished with batch 2019-05-01-----8.735660936996283 s\n",
      "Start with day-----2019-05-02\n",
      "   Finished with batch 2019-05-02-----9.791167069997755 s\n",
      "Start with day-----2019-05-03\n",
      "   Finished with batch 2019-05-03-----6.387965563000762 s\n",
      "Start with day-----2019-05-06\n",
      "   Finished with batch 2019-05-06-----12.704491977005091 s\n",
      "Start with day-----2019-05-07\n",
      "   Finished with batch 2019-05-07-----24.91407797500142 s\n",
      "Start with day-----2019-05-08\n",
      "   Finished with batch 2019-05-08-----25.221295771996665 s\n",
      "Start with day-----2019-05-09\n",
      "   Finished with batch 2019-05-09-----15.799142822994327 s\n",
      "Start with day-----2019-05-10\n",
      "   Finished with batch 2019-05-10-----21.266363536000426 s\n",
      "Start with day-----2019-05-13\n",
      "   Finished with batch 2019-05-13-----17.271776433000923 s\n",
      "Start with day-----2019-05-14\n",
      "   Finished with batch 2019-05-14-----8.547524739005894 s\n",
      "Start with day-----2019-05-15\n",
      "   Finished with batch 2019-05-15-----10.484908733000339 s\n",
      "Start with day-----2019-05-16\n",
      "   Finished with batch 2019-05-16-----6.950616572001309 s\n",
      "Start with day-----2019-05-17\n",
      "   Finished with batch 2019-05-17-----12.406874786000117 s\n",
      "Start with day-----2019-05-20\n",
      "   Finished with batch 2019-05-20-----12.274118040004396 s\n",
      "Start with day-----2019-05-21\n",
      "   Finished with batch 2019-05-21-----7.824348002002807 s\n",
      "Start with day-----2019-05-22\n",
      "   Finished with batch 2019-05-22-----7.5049835559984786 s\n",
      "Start with day-----2019-05-23\n",
      "   Finished with batch 2019-05-23-----12.69917881499714 s\n",
      "Start with day-----2019-05-24\n",
      "   Finished with batch 2019-05-24-----7.739578818000155 s\n",
      "Start with day-----2019-05-28\n",
      "   Finished with batch 2019-05-28-----8.97930063700187 s\n",
      "Start with day-----2019-05-29\n",
      "   Finished with batch 2019-05-29-----11.476334254999529 s\n",
      "Start with day-----2019-05-30\n",
      "   Finished with batch 2019-05-30-----8.133494382003846 s\n",
      "Start with day-----2019-05-31\n",
      "   Finished with batch 2019-05-31-----10.682907529000659 s\n",
      "CPU times: user 4min 25s, sys: 1.51 s, total: 4min 27s\n",
      "Wall time: 4min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for csv_file in csv_files:\n",
    "    start = timer()\n",
    "    print(f\"Start with day-----{csv_file[-12:-8]}-{csv_file[-8:-6]}-{csv_file[-6:-4]}\")\n",
    "    data_df = pd.read_csv(csv_file)\n",
    "    data_df.DT = pd.to_datetime(data_df.DT)\n",
    "    data_df.sort_values(by=['DT'], inplace=True)\n",
    "\n",
    "    # non zero quotes\n",
    "    data_df = data_df.loc[(data_df.BID>0) & (data_df.BIDSIZ>0) & (data_df.ASK>0) & (data_df.ASKSIZ>0)]\n",
    "\n",
    "    # autoselect exchange\n",
    "    data_df['total_size'] = data_df.BIDSIZ + data_df.ASKSIZ\n",
    "    data_df = data_df.loc[data_df.EX == data_df.groupby(['EX']).sum().total_size.idxmax()]\n",
    "\n",
    "    # delete negative spreads\n",
    "    data_df = data_df.loc[data_df.ASK > data_df.BID]\n",
    "\n",
    "    # mergeQuotesSameTimestamp\n",
    "    ex = data_df.EX.values[0]\n",
    "    sym_root = data_df.SYM_ROOT.values[0]\n",
    "    data_df.drop(columns=['SYM_SUFFIX', 'total_size'], inplace=True)\n",
    "    data_df = data_df.groupby(['DT']).median()\n",
    "    data_df['EX'] = ex\n",
    "    data_df['SYM_ROOT'] = sym_root\n",
    "    data_df.reset_index(drop=False, inplace=True)\n",
    "\n",
    "    # remove entries with spread > 50 * daily median spread\n",
    "    data_df['SPREAD'] = data_df.ASK - data_df.BID\n",
    "    data_df = data_df.loc[data_df['SPREAD'] < 50 * data_df['SPREAD'].median()]\n",
    "\n",
    "    # remove outliers using the centered rolling window approach \n",
    "    def compute_diff(x):\n",
    "        return x.values[window] - np.median(np.delete(x.values,window))\n",
    "\n",
    "    window = 25\n",
    "    data_df.sort_values(by=['DT'], inplace=True)\n",
    "    data_df['SPREAD_DIFF'] = data_df.SPREAD.rolling(2*window+1, min_periods=2*window+1, center=True).apply(compute_diff)\n",
    "    data_df = data_df.dropna()\n",
    "    data_df = data_df.loc[data_df['SPREAD_DIFF'] < 10 * data_df['SPREAD_DIFF'].mean()]\n",
    "    data_df = data_df.reset_index(drop=True)\n",
    "\n",
    "    # resample data to 10 minute level\n",
    "    data_df.set_index(['DT'], inplace=True)\n",
    "    data_df[\"MID\"] = data_df.apply(lambda x: (x.ASK * x.ASKSIZ + x.BID * x.BIDSIZ) / (x.ASKSIZ + x.BIDSIZ), axis=1)\n",
    "    data_df = data_df[['MID', 'SYM_ROOT']]\n",
    "    data_df = data_df.resample('5min').last()\n",
    "    #data_df['RETURN'] = data_df.MID.pct_change()\n",
    "    #data_df = data_df.iloc[1:,:]\n",
    "    data_df.reset_index(drop=False, inplace=True)\n",
    "    data_df.DT = data_df.DT.shift(-1) # so that prices are alligned with the time they appear in the market and not with the 5 minutes group\n",
    "    data_df.iloc[-1,0] = data_df.iloc[-2,0] + datetime.timedelta(minutes=5)\n",
    "    data_df.to_sql(data_folder[-3:], db_engine, index=False, if_exists='append')\n",
    "    end = timer()\n",
    "    print(f\"   Finished with batch {csv_file[-12:-8]}-{csv_file[-8:-6]}-{csv_file[-6:-4]}-----{end - start} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graduate-forest",
   "metadata": {},
   "source": [
    "# Create returns tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "blank-ballot",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EEM', 'EZU', 'IWM', 'SPY', 'VEA']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_names = db_engine.table_names()\n",
    "table_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "general-authority",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for table in table_names:\n",
    "    price_data_ticker = pd.read_sql(f\"select * from {table}\", db_engine)\n",
    "    if 'price_data' in locals():\n",
    "        ticker = price_data_ticker.SYM_ROOT[0]\n",
    "        price_data[f\"{ticker}\"] = price_data_ticker.MID\n",
    "    else:\n",
    "        price_data = price_data_ticker[['DT', 'MID']]\n",
    "        price_data.rename(columns={\"MID\": f\"{ticker}\"})\n",
    "\n",
    "# compute log returns and upload to database\n",
    "price_data.set_index(['DT'], inplace=True)\n",
    "return_data = price_data.pct_change().iloc[1:,:]\n",
    "return_data = return_data.apply(np.vectorize(lambda x: np.log(1+x)))\n",
    "return_data.reset_index(drop=False, inplace=True)\n",
    "return_data.to_sql(\"returns\", db_engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nervous-cleaner",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
